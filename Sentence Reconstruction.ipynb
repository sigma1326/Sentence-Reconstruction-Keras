{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Sentence Reconstruction",
   "id": "4bcfba8468dd1012"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The purpose of this project is to take in input a sequence of words corresponding to a random permutation of a given english sentence, and reconstruct the original sentence.\n",
    "\n",
    "The output can be either produced in a single shot, or through an iterative (autoregressive) loop generating a single token at a time.\n",
    "\n",
    "\n",
    "CONSTRAINTS:\n",
    "* No pretrained model can be used.\n",
    "* The neural network models should have less the 20M parameters.\n",
    "* No postprocessing should be done (e.g. no beamsearch)\n",
    "* You cannot use additional training data.\n",
    "\n",
    "\n",
    "BONUS PARAMETERS:\n",
    "\n",
    "A bonus of 0-2 points will be attributed to incentivate the adoption of models with a low number of parameters."
   ],
   "id": "d4ebadee80173717"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " To achieve the project's objectives, I utilized the foundational Transformer model as a base but made adjustments to its parameters and added extra layers for enhanced performance.",
   "id": "9fe753cdab079022"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Import the 'os' module which provides functions to interact with the operating system.",
   "id": "55222f77fb9e787c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T17:45:30.725189Z",
     "start_time": "2024-06-13T17:45:30.721787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# KERAS_BACKEND: Specifies the backend engine to use in Keras, in this case \"tensorflow\".\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "# TF_ENABLE_ONEDNN_OPTS: Enables or disables optimizations provided by oneDNN (Intel's Deep Neural Network library) in TensorFlow.\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "\n",
    "# TF_CPP_MIN_LOG_LEVEL: Sets the minimum severity of log messages displayed by TensorFlow during runtime. By setting this variable to \"3\", only fatal errors will be printed out, and all other informational, warning, or debugging messages will be suppressed. This can make it easier to focus on critical issues without being overwhelmed by verbose output.\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\""
   ],
   "id": "3e6929a1302cbd27",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "All necessary packages are imported at the beginning of the notebook",
   "id": "91c21babc1237bff"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-13T17:45:33.393971Z",
     "start_time": "2024-06-13T17:45:30.920452Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import collections\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras import Model\n",
    "from keras import utils\n",
    "from keras.src.callbacks import EarlyStopping\n",
    "from keras.src.layers import TextVectorization, Dense, Input, LayerNormalization\n",
    "from keras.src.metrics import SparseCategoricalAccuracy\n",
    "from keras.src.optimizers.schedules.learning_rate_schedule import LearningRateSchedule\n",
    "from keras.src.utils import pad_sequences\n",
    "from keras_nlp.src.layers import TransformerEncoder, TokenAndPositionEmbedding, TransformerDecoder\n",
    "from datasets import load_dataset\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "np.random.seed(42)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T17:45:33.397480Z",
     "start_time": "2024-06-13T17:45:33.394799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Tensorflow: {tf.__version__}\")\n",
    "print(f\"Keras: {keras.__version__}\")\n",
    "print(f\"Numpy: {np.__version__}\")"
   ],
   "id": "7efe2002007ee850",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow: 2.16.1\n",
      "Keras: 3.3.3\n",
      "Numpy: 1.26.4\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Define the parameters and variables of the proposed model.",
   "id": "df399e232e6d6352"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T17:45:33.407814Z",
     "start_time": "2024-06-13T17:45:33.398127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "VOCAB_SIZE = 10000\n",
    "SPLIT_INDEX = 220000\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "EMBEDDING_DIM = 128\n",
    "\n",
    "ENCODER_LAYERS_COUNT = 6\n",
    "DECODER_LAYERS_COUNT = 6\n",
    "INTERMEDIATE_DIM = 2048\n",
    "ATTENTION_HEAD_COUNT = 10"
   ],
   "id": "3fd15b9efbae126",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Load and filter the 'generics_kb' dataset.\n",
    "\n",
    "This code loads the 'train' split of the 'generics_kb' dataset, filters out examples where the length of the 'generic_sentence' is less than or equal to 8 words,and returns a list containing only the filtered 'generic_sentence' field.\n"
   ],
   "id": "1271a2cbcc353e63"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T17:45:39.768707Z",
     "start_time": "2024-06-13T17:45:33.408840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = load_dataset('generics_kb', trust_remote_code=True)['train']\n",
    "dataset = dataset.filter(lambda row: len(row[\"generic_sentence\"].split(\" \")) > 8)\n",
    "dataset = dataset['generic_sentence']"
   ],
   "id": "7aca4525a9a4a9f3",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T17:45:39.810126Z",
     "start_time": "2024-06-13T17:45:39.806155Z"
    }
   },
   "cell_type": "code",
   "source": "len(dataset)",
   "id": "7274808a5796d8ba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "462393"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Process a dataset by adding start and end tokens and replacing commas with a comma token.\n",
   "id": "a86f1163eb86b7bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T17:45:40.023702Z",
     "start_time": "2024-06-13T17:45:39.810813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corpus = ['<start> ' + row.replace(\",\", \" <comma>\") + ' <end>' for row in dataset]\n",
    "corpus = np.array(corpus)\n",
    "len(corpus)"
   ],
   "id": "720dac3ad6e98e23",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "462393"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T17:45:40.027334Z",
     "start_time": "2024-06-13T17:45:40.024388Z"
    }
   },
   "cell_type": "code",
   "source": "corpus[0]",
   "id": "922aa2e1cae084dc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> AA batteries maintain the settings if the power ever goes off. <end>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Tokenize a corpus of text using Keras TextVectorization.\n",
    "\n",
    "This function creates a TextVectorization layer from Keras and uses it to adapt and transform\n",
    "the provided corpus into tokenized sequences. The TextVectorization layer standardizes the input\n",
    "by converting text to lowercase and stripping punctuation.\n",
    "\n",
    "Parameters:\n",
    "- corpus : list of str\n",
    "   A list of strings representing the corpus of text to be tokenized.\n",
    "- vocab_size : int, optional\n",
    "   The maximum number of words to keep in the vocabulary, based on word frequency. Default is 10000.\n"
   ],
   "id": "c0483fbe98b8c3cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T17:45:43.196378Z",
     "start_time": "2024-06-13T17:45:40.027869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = TextVectorization(max_tokens=VOCAB_SIZE, standardize=\"lower_and_strip_punctuation\", encoding=\"utf-8\")\n",
    "tokenizer.adapt(corpus)\n",
    "\n",
    "tokenized_sentences = tokenizer(corpus).numpy()\n",
    "len(tokenized_sentences)"
   ],
   "id": "98c068c9cd824a2e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "462393"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T17:45:43.199638Z",
     "start_time": "2024-06-13T17:45:43.197074Z"
    }
   },
   "cell_type": "code",
   "source": "tokenized_sentences[0]",
   "id": "b3ac02b1f002e6bb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   3,    1, 2828,  697,    4, 2794,  161,    4,  175, 2004, 1381,\n",
       "        269,    2,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "84e7507103ea6b25"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code removes rows from a two-dimensional NumPy array `tokenized_sentences` that contain at least one occurrence of the value `1`. The removal is based on a boolean mask generated using the condition `(tokenized_sentences == 1)`, which returns an array with the same shape as `tokenized_sentences` but filled with `True` where the condition is met. The `np.sum()` function then sums up these `True` values along each row (axis=1), and the condition `>= 1` generates a boolean mask that indicates which rows to keep or remove. Finally, `np.delete()` removes the rows where the mask is `True`, effectively removing all rows with at least one occurrence of `1`.",
   "id": "9ca8fbd6d09d232a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T17:45:43.243966Z",
     "start_time": "2024-06-13T17:45:43.201271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mask = np.sum((tokenized_sentences == 1), axis=1) >= 1\n",
    "tokenized_sentences = np.delete(tokenized_sentences, mask, axis=0)\n",
    "\n",
    "tokenized_sentences.shape"
   ],
   "id": "77e9af46d00145e6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(241236, 28)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T17:45:43.246987Z",
     "start_time": "2024-06-13T17:45:43.244591Z"
    }
   },
   "cell_type": "code",
   "source": "tokenized_sentences[0]",
   "id": "3e0ecae5ba109242",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   3, 8948,   30,  254, 1781,   98,    7,   17, 4502,    9,  608,\n",
       "          7,  126,   13,   29,   14, 2901,   15, 1818,    6,  789,    2,\n",
       "          0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T17:45:43.269133Z",
     "start_time": "2024-06-13T17:45:43.247678Z"
    }
   },
   "cell_type": "code",
   "source": "len(tokenizer.get_vocabulary())",
   "id": "dede48d5e97383c7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " # TextDetokenizer Class Documentation\n",
    "\n",
    "The `TextDetokenizer` class is a utility designed to convert tokenized text back into readable strings.\n",
    "\n",
    "\n",
    "## Attributes\n",
    "- **vectorize_layer**: An instance of the vectorization layer used to tokenize text data.\n",
    "- **index_to_word**: A dictionary mapping each index in the vocabulary to its corresponding word.\n",
    "\n",
    "## Methods\n",
    "\n",
    "### `__init__(self, vectorize_layer)`\n",
    "Initializes a new TextDetokenizer instance with the specified vectorization layer. Generates an index-to-word mapping for detokenization.\n",
    "\n",
    "#### Parameters\n",
    "- **vectorize_layer**: The vectorization layer used to tokenize text data. This is required for generating the index-to-word mapping.\n",
    "\n",
    "### `__detokenize_tokens(self, tokens)` (Private Method)\n",
    "Converts a sequence of tokens back into readable text. Recognizes special tokens such as start/end tokens and commas.\n",
    "\n",
    "#### Parameters\n",
    "- **tokens**: A sequence of token indices to be detokenized.\n",
    "\n",
    "#### Returns\n",
    "A string representation of the input tokens.\n",
    "\n",
    "### `__call__(self, batch_tokens)`\n",
    "Enables the TextDetokenizer instance to be called as a function, allowing for easy use in processing batches of text data.\n",
    "\n",
    "#### Parameters\n",
    "- **batch_tokens**: A list of token sequences to be detokenized. Each sequence is converted into a readable string and collected into a new list.\n",
    "\n",
    "#### Returns\n",
    "A list of strings representing the input batch of tokens."
   ],
   "id": "3537b1089a89a828"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T17:45:43.284487Z",
     "start_time": "2024-06-13T17:45:43.269820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TextDetokenizer:\n",
    "    def __init__(self, vectorize_layer):\n",
    "        self.vectorize_layer = vectorize_layer\n",
    "        vocab = self.vectorize_layer.get_vocabulary()\n",
    "        self.index_to_word = {index: word for index, word in enumerate(vocab)}\n",
    "\n",
    "    def __detokenize_tokens(self, tokens):\n",
    "        def check_token(t):\n",
    "            if t == 3:\n",
    "                s = \"<start>\"\n",
    "            elif t == 2:\n",
    "                s = \"<end>\"\n",
    "            elif t == 7:\n",
    "                s = \"<comma>\"\n",
    "            else:\n",
    "                s = self.index_to_word.get(t, '[UNK]')\n",
    "            return s\n",
    "\n",
    "        return ' '.join([check_token(token) for token in tokens if token != 0])\n",
    "\n",
    "    def __call__(self, batch_tokens):\n",
    "        return [self.__detokenize_tokens(tokens) for tokens in batch_tokens]\n",
    "\n",
    "\n",
    "detokenizer = TextDetokenizer(tokenizer)"
   ],
   "id": "fa320d43d38ff20c",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T17:45:43.287666Z",
     "start_time": "2024-06-13T17:45:43.285160Z"
    }
   },
   "cell_type": "code",
   "source": "tokenized_sentences[0]",
   "id": "71aec38d1d4f34db",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   3, 8948,   30,  254, 1781,   98,    7,   17, 4502,    9,  608,\n",
       "          7,  126,   13,   29,   14, 2901,   15, 1818,    6,  789,    2,\n",
       "          0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T17:45:43.296821Z",
     "start_time": "2024-06-13T17:45:43.288434Z"
    }
   },
   "cell_type": "code",
   "source": "detokenizer([tokenized_sentences[0]])",
   "id": "b8dceae1ce2ab19f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> aardvarks also require sandy soil <comma> as opposed to rocks <comma> so that they can dig for termites and ants <end>']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "dc7f31d39b4812a0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code creates a copy of an existing data structure, `tokenized_sentences`, and assigns it to a new variable, `original_data`. The purpose of this operation is to preserve the original data while any modifications are made to the copied version (`original_data`). After the copying process, the shape of the `original_data` is checked and displayed using the `shape` attribute.",
   "id": "ef1cf144830cb620"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T17:45:43.315766Z",
     "start_time": "2024-06-13T17:45:43.297521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "original_data = tokenized_sentences.copy()\n",
    "original_data.shape"
   ],
   "id": "73d1ea90f0e85adf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(241236, 28)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code generates a random permutation of the input data",
   "id": "5288be52ad0b2f5e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T17:45:43.342231Z",
     "start_time": "2024-06-13T17:45:43.316378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Make a random permutation of training and test set\n",
    "# Shuffle the all data\n",
    "\n",
    "shuffled_indices = np.random.permutation(len(original_data))\n",
    "shuffled_data = original_data[shuffled_indices]\n",
    "\n",
    "shuffled_data.shape"
   ],
   "id": "5dc8a992aebaf4b6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(241236, 28)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T17:45:43.345020Z",
     "start_time": "2024-06-13T17:45:43.343098Z"
    }
   },
   "cell_type": "code",
   "source": "original_data = shuffled_data",
   "id": "228c95aff190ac5c",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code segment prepares target data for further processing by removing the '<start>' token and padding sequences.\n",
    "\n",
    "Notes:\n",
    "- The padding value used is 'post', which means that padding will be added at the end of each sequence.\n"
   ],
   "id": "27762e603fbce9e0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T17:45:43.698068Z",
     "start_time": "2024-06-13T17:45:43.345740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "target_data = np.array([s[1:] for s in shuffled_data])  # copy of the original data without the <start> token\n",
    "target_data = pad_sequences(target_data, maxlen=shuffled_data.shape[1], padding='post')\n",
    "\n",
    "original_data.shape, target_data.shape"
   ],
   "id": "27e47181b77d7d1c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((241236, 28), (241236, 28))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T17:45:43.700822Z",
     "start_time": "2024-06-13T17:45:43.698711Z"
    }
   },
   "cell_type": "code",
   "source": "original_data[0]",
   "id": "ad383f9601c4b228",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   3, 1553,   14, 1565, 1174,  193, 1380,    9,   10,  217,    6,\n",
       "        164,   29, 2732,   20,   91,  158,    2,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T17:45:43.711884Z",
     "start_time": "2024-06-13T17:45:43.701633Z"
    }
   },
   "cell_type": "code",
   "source": "target_data[0]",
   "id": "e21aa85729edbefa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1553,   14, 1565, 1174,  193, 1380,    9,   10,  217,    6,  164,\n",
       "         29, 2732,   20,   91,  158,    2,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code is used to split a dataset into training and testing subsets",
   "id": "3a9214e69ee9532"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T17:45:43.721494Z",
     "start_time": "2024-06-13T17:45:43.712500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "original_data_train, original_data_test = original_data[:SPLIT_INDEX], original_data[SPLIT_INDEX:]\n",
    "original_data_train.shape, original_data_test.shape"
   ],
   "id": "f97d29b62c55bb41",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((220000, 28), (21236, 28))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T17:45:43.730958Z",
     "start_time": "2024-06-13T17:45:43.722155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "target_data_train, target_data_test = target_data[:SPLIT_INDEX], target_data[SPLIT_INDEX:],\n",
    "target_data_train.shape, target_data_test.shape"
   ],
   "id": "699ab8dc7c65cae",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((220000, 28), (21236, 28))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T17:45:43.739963Z",
     "start_time": "2024-06-13T17:45:43.731474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MAX_SEQUENCE_LENGTH = original_data.shape[1]\n",
    "\n",
    "MAX_SEQUENCE_LENGTH"
   ],
   "id": "18f0520abe53435",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T17:45:43.749509Z",
     "start_time": "2024-06-13T17:45:43.740500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DataGenerator(utils.Sequence):\n",
    "    \"\"\"A data generator class that inherits from `keras.utils.Sequence`.\n",
    "\n",
    "   This class is used to create batches of data for training a model. It shuffles a portion\n",
    "   of each batch's input data, creates padding masks, and generates the corresponding target data.\n",
    "\n",
    "   Parameters:\n",
    "   ----------\n",
    "       data : np.ndarray\n",
    "           The input data. Each row represents an instance or sequence.\n",
    "       target_data_ : np.ndarray\n",
    "           The target data. Each element corresponds to the label of the corresponding input sequence.\n",
    "       batch_size : int, optional (default=32)\n",
    "           The size of each batch to generate.\n",
    "       shuffle : bool, optional (default=True)\n",
    "           Whether to shuffle the data after each epoch.\n",
    "       seed : int, optional (default=42)\n",
    "           The random seed used for shuffling the data.\n",
    "   \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            data: np.ndarray,\n",
    "            target_data_: np.ndarray,\n",
    "            batch_size=32,\n",
    "            shuffle: bool = True,\n",
    "            seed: int = 42,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.target_data = target_data_\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "        self.use_multiprocessing = True\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of batches in the data generator.\"\"\"\n",
    "        return int(np.floor(len(self.data) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Generates a batch of data at the given index.\n",
    "\n",
    "       Parameters:\n",
    "       ----------\n",
    "       index : int\n",
    "           The index of the batch to generate.\n",
    "\n",
    "       Returns:\n",
    "       -------\n",
    "       tuple\n",
    "           A tuple containing the input data and padding mask, as well as the target data.\n",
    "\n",
    "       Raises:\n",
    "       ------\n",
    "       StopIteration\n",
    "           If the index is out of range.\n",
    "       \"\"\"\n",
    "        # this is to ensure that the iterator will stop when the index is out of range.\n",
    "        if index * self.batch_size > len(self.data):\n",
    "            raise StopIteration\n",
    "\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        data_batch = np.array([self.data[k] for k in indexes])\n",
    "        target_data_ = np.array([self.target_data[k] for k in indexes])\n",
    "\n",
    "        # copy of ordered sequences\n",
    "        ordered_data = np.copy(data_batch)\n",
    "\n",
    "        #shuffle only the relevant positions for each batch\n",
    "        for i in range(data_batch.shape[0]):\n",
    "            np.random.shuffle(data_batch[i, 1:data_batch[i].argmin() - 1])\n",
    "\n",
    "        pad_mask = np.where(data_batch == 0, 0, 1)\n",
    "\n",
    "        return (data_batch, ordered_data, pad_mask, pad_mask), target_data_\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Shuffles the data after each epoch if `shuffle` is True.\"\"\"\n",
    "        self.indexes = np.arange(len(self.data))\n",
    "    \n",
    "        if self.shuffle:\n",
    "            if self.seed is not None:\n",
    "                np.random.seed(self.seed)\n",
    "            np.random.shuffle(self.indexes)"
   ],
   "id": "5dfcf0c1109f762a",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Splits the given dataset into training and testing sets using a custom data generator.",
   "id": "207b3f3f9ba76f60"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T17:45:43.762654Z",
     "start_time": "2024-06-13T17:45:43.750154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# split the dataset into training and testing sets\n",
    "\n",
    "train_generator = DataGenerator(shuffled_data[:SPLIT_INDEX], target_data[:SPLIT_INDEX], BATCH_SIZE)\n",
    "test_generator = DataGenerator(shuffled_data[SPLIT_INDEX:], target_data[SPLIT_INDEX:], BATCH_SIZE)"
   ],
   "id": "8cc712fdcd589b01",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T17:45:43.772581Z",
     "start_time": "2024-06-13T17:45:43.763332Z"
    }
   },
   "cell_type": "code",
   "source": "shuffled_data.shape",
   "id": "cfcf0b82452d2374",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(241236, 28)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "The `TransformerPaperLearningRateSchedule` class implements a custom learning rate schedule as used in the original Transformer paper by Vaswani et al. (2017). The learning rate schedule is designed to help with model convergence and improve training stability during the initial stages of training, when the model parameters are still changing significantly.\n",
    "\n",
    "## Attributes\n",
    "- `model_dimension`: An integer representing the dimensionality of the model's embeddings. This value is used in the calculation of the learning rate.\n",
    "- `warmup_period`: An integer that determines the number of training steps during which the learning rate increases linearly from zero to its initial value. Defaults to 1000.\n",
    "\n",
    "## Methods\n",
    "### `__call__(self, current_step)`\n",
    "This method calculates and returns the learning rate for a given step in training, based on the formula used in the original Transformer paper:\n",
    "\n",
    "```python\n",
    "learning_rate = scale_factor * min(1 / sqrt(current_step), current_step / (warmup_period ** 1.5))\n",
    "```\n",
    "\n",
    "where `scale_factor` is calculated as `sqrt(model_dimension)`. The learning rate starts at zero and increases during the warm-up period, after which it decreases according to a square root schedule. This ensures that the learning rate remains small when the current step number is large, allowing for more fine-grained updates to the model parameters as training progresses.\n",
    "\n",
    "### `get_config(self)`\n",
    "This method returns a dictionary containing the configuration of the learning rate schedule object, including the values of the `model_dimension` and `warmup_period` attributes. This can be useful for saving or loading the learning rate schedule object from disk, as it allows you to recreate an identical object with the same configuration settings."
   ],
   "id": "10704971c5d34247"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T17:45:43.782004Z",
     "start_time": "2024-06-13T17:45:43.774351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerPaperLearningRateSchedule(LearningRateSchedule):\n",
    "    def __init__(self, model_dimension, warmup_period=1000):\n",
    "        super(TransformerPaperLearningRateSchedule, self).__init__()\n",
    "        self.model_dimension = model_dimension\n",
    "        self.warmup_period = warmup_period\n",
    "\n",
    "    def __call__(self, current_step):\n",
    "        current_step = tf.cast(current_step, dtype=tf.float32)\n",
    "        scale_factor = tf.math.rsqrt(tf.cast(self.model_dimension, tf.float32))\n",
    "        return scale_factor * tf.math.minimum(\n",
    "                tf.math.rsqrt(current_step),\n",
    "                tf.cast(current_step * (self.warmup_period ** -1.5), dtype=tf.float32),\n",
    "        )\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'model_dimension': self.model_dimension,\n",
    "            'warmup_period': self.warmup_period\n",
    "        }"
   ],
   "id": "9b0e976fa687a0fa",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Construct a transformer-based sequence-to-sequence model with specified dimensions, layers counts, intermediate dimension, and attention heads count. This model is intended for tasks such as machine translation or text summarization. The function uses a token embedding and positional encoding mechanism to represent input sequences and apply multi-head self-attention and cross-attention mechanisms in both the encoder and decoder components of the transformer architecture.\n",
    "\n",
    "### Parameters:\n",
    "- `embedding_dim` (int): The dimension size for word embeddings.\n",
    "- `encoder_layers_count` (int): The number of transformer encoder layers to include in the model.\n",
    "- `decoder_layers_count` (int): The number of transformer decoder layers to include in the model.\n",
    "- `intermediate_dim` (int): The dimension size for feedforward networks within each encoder and decoder layer.\n",
    "- `attention_heads_count` (int): The number of attention heads to use in both the multi-head self-attention and cross-attention mechanisms.\n",
    "\n",
    "### Returns:\n",
    "- A compiled Keras Model instance that accepts inputs for an encoder sequence, a decoder sequence, an encoder padding mask, and a decoder padding mask; this model outputs a probability distribution over target vocabulary words at each position in the output sequence.\n",
    "\n",
    "The function begins by defining input layers for the encoder and decoder sequences as well as their respective padding masks. It then creates token embedding\n",
    " layers with positional encoding for both the encoder and decoder sequences. Subsequently, it applies a stack of transformer encoder layers to process the encoded input sequence while accounting for padding. Next, the function builds a stack of transformer decoder layers to generate an output sequence from the encoder outputs while attending to both the encoded input sequence and previous positions in the decoded output sequence, also taking into account padding. After the final decoder layer, a normalization layer is applied followed by a dense layer with softmax activation to produce probability distributions over target vocabulary words for each position in the output sequence. The function returns this compiled model instance."
   ],
   "id": "21d7912949d0d450"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T17:45:43.792221Z",
     "start_time": "2024-06-13T17:45:43.782601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_model(\n",
    "        embedding_dim: int,\n",
    "        encoder_layers_count: int,\n",
    "        decoder_layers_count: int,\n",
    "        intermediate_dim: int,\n",
    "        attention_heads_count: int\n",
    ") -> Model:\n",
    "    # Defining the inputs of the neural network\n",
    "    encoder_input = Input(\n",
    "            shape=(MAX_SEQUENCE_LENGTH,),\n",
    "            dtype=\"int32\",\n",
    "            name=\"encoder_input\",\n",
    "    )\n",
    "    decoder_input = Input(\n",
    "            shape=(MAX_SEQUENCE_LENGTH,),\n",
    "            dtype=\"int32\",\n",
    "            name=\"decoder_input\",\n",
    "    )\n",
    "\n",
    "    encoder_padding_mask = Input(\n",
    "            shape=(MAX_SEQUENCE_LENGTH,),\n",
    "            dtype=\"int32\",\n",
    "            name=\"encoder_padding_mask\",\n",
    "    )\n",
    "    decoder_padding_mask = Input(\n",
    "            shape=(MAX_SEQUENCE_LENGTH,),\n",
    "            dtype=\"int32\",\n",
    "            name=\"decoder_padding_mask\",\n",
    "    )\n",
    "\n",
    "    # Defining the encoder layers of the neural network\n",
    "    encoder_embedding = TokenAndPositionEmbedding(\n",
    "            vocabulary_size=10000,\n",
    "            sequence_length=32,\n",
    "            embedding_dim=embedding_dim,\n",
    "            name=\"encoder_embedding\",\n",
    "    )(encoder_input)\n",
    "\n",
    "    encoder_x = encoder_embedding\n",
    "    for i in range(encoder_layers_count):\n",
    "        encoder_layer = TransformerEncoder(\n",
    "                num_heads=attention_heads_count,\n",
    "                intermediate_dim=intermediate_dim,\n",
    "                dropout=0.1,\n",
    "                name=f\"encoder_{i}\",\n",
    "        )\n",
    "        encoder_x = encoder_layer(encoder_x, padding_mask=encoder_padding_mask)\n",
    "\n",
    "    # Defining the decoder layers of the neural network\n",
    "    decoder_embedding = TokenAndPositionEmbedding(\n",
    "            vocabulary_size=VOCAB_SIZE,\n",
    "            sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "            embedding_dim=embedding_dim,\n",
    "            name=\"decoder_embedding\",\n",
    "    )(decoder_input)\n",
    "\n",
    "    decoder_x = decoder_embedding\n",
    "    for i in range(decoder_layers_count):\n",
    "        decoder_layer = TransformerDecoder(\n",
    "                num_heads=attention_heads_count,\n",
    "                intermediate_dim=intermediate_dim,\n",
    "                dropout=0.1,\n",
    "                name=f\"decoder_{i}\"\n",
    "        )\n",
    "        decoder_x = decoder_layer(\n",
    "                encoder_sequence=encoder_x,\n",
    "                decoder_sequence=decoder_x,\n",
    "                decoder_padding_mask=decoder_padding_mask,\n",
    "                encoder_padding_mask=encoder_padding_mask,\n",
    "        )\n",
    "\n",
    "    # Normalization before final Dense layer\n",
    "    decoder_x = LayerNormalization(epsilon=1e-6)(decoder_x)\n",
    "\n",
    "    # Defining the output of the neural network\n",
    "    outputs = Dense(VOCAB_SIZE, activation=\"softmax\")(decoder_x)\n",
    "\n",
    "    return Model(\n",
    "            inputs=[\n",
    "                encoder_input,\n",
    "                decoder_input,\n",
    "                encoder_padding_mask,\n",
    "                decoder_padding_mask,\n",
    "            ],\n",
    "            outputs=outputs,\n",
    "    )"
   ],
   "id": "e4abfc56558d5ed2",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This segment of the code performs three primary tasks: defining the learning rate schedule, metric, and optimizer for the transformer model; setting up an \n",
    "early stopping mechanism to prevent overfitting during training; and finally, building and compiling the model with the previously defined parameters.\n",
    "\n",
    "### Learning Rate Schedule\n",
    "- A `TransformerPaperLearningRateSchedule` is instantiated with a specified embedding dimension (`EMBEDDING_DIM`) and warmup period (1000).\n",
    "\n",
    "### Metric, Optimizer, and Early Stopping Callback\n",
    "- The `SparseCategoricalAccuracy` metric is defined for the model's performance evaluation.\n",
    "- An `Adam` optimizer with customizable parameters (learning rate from the previous step, beta coefficients, and epsilon) is created. Adam is an efficient stochastic optimization algorithm that works well in practice.\n",
    "- An early stopping mechanism is set up to stop training if the validation loss does not improve for a certain number of epochs (patience=5). This helps prevent overfitting by halting training when performance on unseen data begins to degrade.\n",
    "\n",
    "### Model Building and Compilation\n",
    "- A transformer model is built using the `build_model` function with parameters such as embedding dimension, number of encoder/decoder layers, intermediate dimension, and attention head count.\n",
    "- The model is then compiled with the previously defined optimizer, loss function (sparse categorical crossentropy), and metric (accuracy).\n",
    "- Finally, a summary of the model's architecture is printed to the console for review."
   ],
   "id": "412e13a79e7916dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T17:45:44.233477Z",
     "start_time": "2024-06-13T17:45:43.792882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "round_one_learning_rate = TransformerPaperLearningRateSchedule(model_dimension=EMBEDDING_DIM, warmup_period=1000)\n",
    "round_one_metric = SparseCategoricalAccuracy(name=\"accuracy\")\n",
    "round_one_optimizer = Adam(\n",
    "        learning_rate=round_one_learning_rate,\n",
    "        beta_1=0.90,\n",
    "        beta_2=0.98,\n",
    "        epsilon=1e-9,\n",
    ")\n",
    "# define a callback to stop the training if the validation loss doesn't come down for a while.\n",
    "round_one_early_stopping_callback = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        verbose=1,\n",
    "        mode=\"min\",\n",
    ")\n",
    "\n",
    "model = build_model(\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        encoder_layers_count=ENCODER_LAYERS_COUNT,\n",
    "        decoder_layers_count=DECODER_LAYERS_COUNT,\n",
    "        intermediate_dim=INTERMEDIATE_DIM,\n",
    "        attention_heads_count=ATTENTION_HEAD_COUNT,\n",
    ")\n",
    "model.compile(\n",
    "        optimizer=round_one_optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[round_one_metric],\n",
    ")\n",
    "\n",
    "model.summary()"
   ],
   "id": "76165fce8e2eae08",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"functional_1\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape     \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m   Param #\u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mConnected to     \u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m28\u001B[0m)        │          \u001B[38;5;34m0\u001B[0m │ -                 │\n",
       "│ (\u001B[38;5;33mInputLayer\u001B[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_padding_ma… │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m28\u001B[0m)        │          \u001B[38;5;34m0\u001B[0m │ -                 │\n",
       "│ (\u001B[38;5;33mInputLayer\u001B[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_embedding   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m28\u001B[0m, \u001B[38;5;34m128\u001B[0m)   │  \u001B[38;5;34m1,284,096\u001B[0m │ encoder_input[\u001B[38;5;34m0\u001B[0m]… │\n",
       "│ (\u001B[38;5;33mTokenAndPositionE…\u001B[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_0           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m28\u001B[0m, \u001B[38;5;34m128\u001B[0m)   │    \u001B[38;5;34m588,904\u001B[0m │ encoder_embeddin… │\n",
       "│ (\u001B[38;5;33mTransformerEncode…\u001B[0m │                   │            │ encoder_padding_… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_1           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m28\u001B[0m, \u001B[38;5;34m128\u001B[0m)   │    \u001B[38;5;34m588,904\u001B[0m │ encoder_0[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m],  │\n",
       "│ (\u001B[38;5;33mTransformerEncode…\u001B[0m │                   │            │ encoder_padding_… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_2           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m28\u001B[0m, \u001B[38;5;34m128\u001B[0m)   │    \u001B[38;5;34m588,904\u001B[0m │ encoder_1[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m],  │\n",
       "│ (\u001B[38;5;33mTransformerEncode…\u001B[0m │                   │            │ encoder_padding_… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_3           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m28\u001B[0m, \u001B[38;5;34m128\u001B[0m)   │    \u001B[38;5;34m588,904\u001B[0m │ encoder_2[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m],  │\n",
       "│ (\u001B[38;5;33mTransformerEncode…\u001B[0m │                   │            │ encoder_padding_… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_input       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m28\u001B[0m)        │          \u001B[38;5;34m0\u001B[0m │ -                 │\n",
       "│ (\u001B[38;5;33mInputLayer\u001B[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_4           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m28\u001B[0m, \u001B[38;5;34m128\u001B[0m)   │    \u001B[38;5;34m588,904\u001B[0m │ encoder_3[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m],  │\n",
       "│ (\u001B[38;5;33mTransformerEncode…\u001B[0m │                   │            │ encoder_padding_… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_embedding   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m28\u001B[0m, \u001B[38;5;34m128\u001B[0m)   │  \u001B[38;5;34m1,283,584\u001B[0m │ decoder_input[\u001B[38;5;34m0\u001B[0m]… │\n",
       "│ (\u001B[38;5;33mTokenAndPositionE…\u001B[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_padding_ma… │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m28\u001B[0m)        │          \u001B[38;5;34m0\u001B[0m │ -                 │\n",
       "│ (\u001B[38;5;33mInputLayer\u001B[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_5           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m28\u001B[0m, \u001B[38;5;34m128\u001B[0m)   │    \u001B[38;5;34m588,904\u001B[0m │ encoder_4[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m],  │\n",
       "│ (\u001B[38;5;33mTransformerEncode…\u001B[0m │                   │            │ encoder_padding_… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_0           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m28\u001B[0m, \u001B[38;5;34m128\u001B[0m)   │    \u001B[38;5;34m651,088\u001B[0m │ decoder_embeddin… │\n",
       "│ (\u001B[38;5;33mTransformerDecode…\u001B[0m │                   │            │ decoder_padding_… │\n",
       "│                     │                   │            │ encoder_padding_… │\n",
       "│                     │                   │            │ encoder_5[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_1           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m28\u001B[0m, \u001B[38;5;34m128\u001B[0m)   │    \u001B[38;5;34m651,088\u001B[0m │ decoder_0[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m],  │\n",
       "│ (\u001B[38;5;33mTransformerDecode…\u001B[0m │                   │            │ decoder_padding_… │\n",
       "│                     │                   │            │ encoder_padding_… │\n",
       "│                     │                   │            │ encoder_5[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_2           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m28\u001B[0m, \u001B[38;5;34m128\u001B[0m)   │    \u001B[38;5;34m651,088\u001B[0m │ decoder_1[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m],  │\n",
       "│ (\u001B[38;5;33mTransformerDecode…\u001B[0m │                   │            │ decoder_padding_… │\n",
       "│                     │                   │            │ encoder_padding_… │\n",
       "│                     │                   │            │ encoder_5[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_3           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m28\u001B[0m, \u001B[38;5;34m128\u001B[0m)   │    \u001B[38;5;34m651,088\u001B[0m │ decoder_2[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m],  │\n",
       "│ (\u001B[38;5;33mTransformerDecode…\u001B[0m │                   │            │ decoder_padding_… │\n",
       "│                     │                   │            │ encoder_padding_… │\n",
       "│                     │                   │            │ encoder_5[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_4           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m28\u001B[0m, \u001B[38;5;34m128\u001B[0m)   │    \u001B[38;5;34m651,088\u001B[0m │ decoder_3[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m],  │\n",
       "│ (\u001B[38;5;33mTransformerDecode…\u001B[0m │                   │            │ decoder_padding_… │\n",
       "│                     │                   │            │ encoder_padding_… │\n",
       "│                     │                   │            │ encoder_5[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_5           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m28\u001B[0m, \u001B[38;5;34m128\u001B[0m)   │    \u001B[38;5;34m651,088\u001B[0m │ decoder_4[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m],  │\n",
       "│ (\u001B[38;5;33mTransformerDecode…\u001B[0m │                   │            │ decoder_padding_… │\n",
       "│                     │                   │            │ encoder_padding_… │\n",
       "│                     │                   │            │ encoder_5[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalization │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m28\u001B[0m, \u001B[38;5;34m128\u001B[0m)   │        \u001B[38;5;34m256\u001B[0m │ decoder_5[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]   │\n",
       "│ (\u001B[38;5;33mLayerNormalizatio…\u001B[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001B[38;5;33mDense\u001B[0m)       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m28\u001B[0m, \u001B[38;5;34m10000\u001B[0m) │  \u001B[38;5;34m1,290,000\u001B[0m │ layer_normalizat… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_padding_ma… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,284,096</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionE…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_0           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">588,904</span> │ encoder_embeddin… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │ encoder_padding_… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">588,904</span> │ encoder_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │ encoder_padding_… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">588,904</span> │ encoder_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │ encoder_padding_… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">588,904</span> │ encoder_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │ encoder_padding_… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">588,904</span> │ encoder_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │ encoder_padding_… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,283,584</span> │ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionE…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_padding_ma… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">588,904</span> │ encoder_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │ encoder_padding_… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_0           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">651,088</span> │ decoder_embeddin… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ decoder_padding_… │\n",
       "│                     │                   │            │ encoder_padding_… │\n",
       "│                     │                   │            │ encoder_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">651,088</span> │ decoder_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ decoder_padding_… │\n",
       "│                     │                   │            │ encoder_padding_… │\n",
       "│                     │                   │            │ encoder_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">651,088</span> │ decoder_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ decoder_padding_… │\n",
       "│                     │                   │            │ encoder_padding_… │\n",
       "│                     │                   │            │ encoder_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">651,088</span> │ decoder_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ decoder_padding_… │\n",
       "│                     │                   │            │ encoder_padding_… │\n",
       "│                     │                   │            │ encoder_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">651,088</span> │ decoder_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ decoder_padding_… │\n",
       "│                     │                   │            │ encoder_padding_… │\n",
       "│                     │                   │            │ encoder_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">651,088</span> │ decoder_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ decoder_padding_… │\n",
       "│                     │                   │            │ encoder_padding_… │\n",
       "│                     │                   │            │ encoder_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ decoder_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290,000</span> │ layer_normalizat… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m11,297,888\u001B[0m (43.10 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,297,888</span> (43.10 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m11,297,888\u001B[0m (43.10 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,297,888</span> (43.10 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T17:45:44.236987Z",
     "start_time": "2024-06-13T17:45:44.234096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def data_generator(ordered_data, target_data_, batch_size_):\n",
    "    \"\"\"\n",
    "    Generator function to produce input-output pairs for training the neural network model.\n",
    "\n",
    "    This generator shuffles the elements between the start and end of each sequence in the\n",
    "    batch (excluding padding tokens), and yields the input-output pair with corresponding masks.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ordered_data : array_like\n",
    "        A 2D array containing input sequences. Each row corresponds to a sequence, and each element of the sequence is an integer token.\n",
    "    target_data_ : array_like\n",
    "        A 2D array containing output sequences (targets) corresponding to the input sequences.\n",
    "    batch_size_ : int\n",
    "        The number of sequences in each batch yielded by the generator.\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    tuple\n",
    "        A tuple containing two elements: a tuple of encoder input, decoder input, padding masks for both inputs and outputs, and a 2D array of one-hot encoded output targets (output_onehot).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This function assumes that the end of each sequence in `ordered_data` is marked by the token '2'.\n",
    "    The sequences are shuffled between the start and the first occurrence of token '2' (exclusive), and then padded with zeros to a maximum length of `batch_size_`.\n",
    "    The padding masks are created such that they have zeros in the positions corresponding to padded tokens, and ones elsewhere.\n",
    "\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        for i in range(0, len(ordered_data), batch_size_):\n",
    "            encoder_input = ordered_data[i:i + batch_size_].copy()\n",
    "            for j in range(len(encoder_input)):\n",
    "                eos_index = np.where(encoder_input[j] == 2)[0][0]\n",
    "                np.random.shuffle(encoder_input[j][1:eos_index])\n",
    "\n",
    "            padding_mask_enc = padding_mask_dec = np.where(encoder_input == 0, 0, 1)\n",
    "            decoder_input = ordered_data[i:i + batch_size_]\n",
    "\n",
    "            yield (encoder_input, decoder_input, padding_mask_enc, padding_mask_dec), target_data_[i:i + batch_size_]\n"
   ],
   "id": "a143c5ba5289e976",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Train a model using data generated from custom generator functions.\n",
    "\n",
    "Parameters:\n",
    "- **model (tf.keras.Model)**: The machine learning model to train.\n",
    "- **original_data_train, target_data_train (array-like)**: Input and output data for the training set.\n",
    "- **original_data_test, target_data_test (array-like)**: Input and output data for the validation set.\n",
    "- **BATCH_SIZE (int)**: The number of samples per gradient update.\n",
    "- **round_one_early_stopping_callback (tf.keras.callbacks.Callback)**: A callback that stops training when a monitored metric has stopped improving.\n"
   ],
   "id": "65571d0bb68585f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:05:22.594587Z",
     "start_time": "2024-06-13T17:45:44.237585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "_ = model.fit(\n",
    "        data_generator(original_data_train, target_data_train, batch_size_=BATCH_SIZE),\n",
    "        epochs=50,\n",
    "        steps_per_epoch=len(original_data_train) // BATCH_SIZE,\n",
    "        validation_data=data_generator(original_data_test, target_data_test, batch_size_=BATCH_SIZE),\n",
    "        validation_steps=5,\n",
    "        shuffle=True,\n",
    "        callbacks=[round_one_early_stopping_callback],\n",
    ")"
   ],
   "id": "91f8787ed1a2069f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1718300761.941169 2788980 service.cc:145] XLA service 0x6075570f9560 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1718300761.941188 2788980 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3080 Laptop GPU, Compute Capability 8.6\n",
      "W0000 00:00:1718300763.677080 2788980 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1718300794.291871 2789570 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_97', 100 bytes spill stores, 92 bytes spill loads\n",
      "\n",
      "I0000 00:00:1718300794.430034 2789572 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_97', 48 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "I0000 00:00:1718300794.477505 2789578 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_97', 284 bytes spill stores, 252 bytes spill loads\n",
      "\n",
      "I0000 00:00:1718300794.500683 2789585 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_97', 304 bytes spill stores, 304 bytes spill loads\n",
      "\n",
      "I0000 00:00:1718300794.511845 2789573 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_97', 672 bytes spill stores, 640 bytes spill loads\n",
      "\n",
      "I0000 00:00:1718300794.858420 2789584 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 464 bytes spill stores, 412 bytes spill loads\n",
      "\n",
      "I0000 00:00:1718300794.932698 2789579 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_502', 44 bytes spill stores, 28 bytes spill loads\n",
      "\n",
      "I0000 00:00:1718300795.693701 2789573 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 192 bytes spill stores, 192 bytes spill loads\n",
      "\n",
      "I0000 00:00:1718300796.376030 2789583 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_411', 176 bytes spill stores, 148 bytes spill loads\n",
      "\n",
      "I0000 00:00:1718300797.021189 2789585 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_411', 28 bytes spill stores, 28 bytes spill loads\n",
      "\n",
      "I0000 00:00:1718300797.193632 2789577 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_411', 412 bytes spill stores, 416 bytes spill loads\n",
      "\n",
      "I0000 00:00:1718300831.764360 2788980 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_43', 40 bytes spill stores, 40 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_41', 484 bytes spill stores, 484 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_33', 4 bytes spill stores, 40 bytes spill loads\n",
      "\n",
      "I0000 00:00:1718300831.835477 2788980 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m429/429\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 168ms/step - accuracy: 0.5102 - loss: 5.9811"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1718300905.342382 2788982 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m429/429\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m162s\u001B[0m 174ms/step - accuracy: 0.5105 - loss: 5.9763 - val_accuracy: 0.7430 - val_loss: 1.7961\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1718300908.062586 2788976 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "I0000 00:00:1718300937.542665 2792973 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_97', 48 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "I0000 00:00:1718300937.545991 2792979 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_97', 148 bytes spill stores, 120 bytes spill loads\n",
      "\n",
      "I0000 00:00:1718300937.694548 2792974 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_97', 672 bytes spill stores, 640 bytes spill loads\n",
      "\n",
      "I0000 00:00:1718300937.771386 2792983 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_502', 220 bytes spill stores, 220 bytes spill loads\n",
      "\n",
      "I0000 00:00:1718300937.841164 2792981 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_97', 304 bytes spill stores, 304 bytes spill loads\n",
      "\n",
      "I0000 00:00:1718300938.406868 2792974 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 192 bytes spill stores, 192 bytes spill loads\n",
      "\n",
      "I0000 00:00:1718300938.526469 2792981 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_97', 284 bytes spill stores, 252 bytes spill loads\n",
      "\n",
      "I0000 00:00:1718300938.697942 2792970 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_502', 392 bytes spill stores, 392 bytes spill loads\n",
      "\n",
      "I0000 00:00:1718300938.945613 2792978 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 464 bytes spill stores, 412 bytes spill loads\n",
      "\n",
      "I0000 00:00:1718300939.269720 2792970 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_502', 52 bytes spill stores, 52 bytes spill loads\n",
      "\n",
      "I0000 00:00:1718300939.874882 2792980 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 28 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "I0000 00:00:1718300940.245255 2792974 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_411', 260 bytes spill stores, 268 bytes spill loads\n",
      "\n",
      "I0000 00:00:1718300973.107600 2788976 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_43', 40 bytes spill stores, 40 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_41', 4040 bytes spill stores, 4208 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function '__cuda_sm3x_div_rn_noftz_f32_slowpath', 52 bytes spill stores, 48 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_33', 4 bytes spill stores, 40 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m429/429\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m140s\u001B[0m 171ms/step - accuracy: 0.7773 - loss: 1.4901 - val_accuracy: 0.8530 - val_loss: 0.8001\n",
      "Epoch 3/50\n",
      "\u001B[1m429/429\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m72s\u001B[0m 167ms/step - accuracy: 0.8592 - loss: 0.7145 - val_accuracy: 0.8874 - val_loss: 0.5078\n",
      "Epoch 4/50\n",
      "\u001B[1m429/429\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m71s\u001B[0m 165ms/step - accuracy: 0.8880 - loss: 0.4806 - val_accuracy: 0.9014 - val_loss: 0.4219\n",
      "Epoch 5/50\n",
      "\u001B[1m429/429\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m73s\u001B[0m 171ms/step - accuracy: 0.9033 - loss: 0.3831 - val_accuracy: 0.9074 - val_loss: 0.3940\n",
      "Epoch 6/50\n",
      "\u001B[1m429/429\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m73s\u001B[0m 171ms/step - accuracy: 0.9140 - loss: 0.3235 - val_accuracy: 0.9091 - val_loss: 0.3763\n",
      "Epoch 7/50\n",
      "\u001B[1m429/429\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m74s\u001B[0m 171ms/step - accuracy: 0.9225 - loss: 0.2810 - val_accuracy: 0.9102 - val_loss: 0.3743\n",
      "Epoch 8/50\n",
      "\u001B[1m429/429\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m72s\u001B[0m 169ms/step - accuracy: 0.9293 - loss: 0.2486 - val_accuracy: 0.9135 - val_loss: 0.3711\n",
      "Epoch 9/50\n",
      "\u001B[1m429/429\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 169ms/step - accuracy: 0.9348 - loss: 0.2238"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1718301553.572553 2788981 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "I0000 00:00:1718301554.439121 2807607 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 264 bytes spill stores, 264 bytes spill loads\n",
      "\n",
      "I0000 00:00:1718301554.529722 2807600 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 28 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "I0000 00:00:1718301554.705888 2807596 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 476 bytes spill stores, 280 bytes spill loads\n",
      "\n",
      "I0000 00:00:1718301554.799675 2807598 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m429/429\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m75s\u001B[0m 176ms/step - accuracy: 0.9348 - loss: 0.2238 - val_accuracy: 0.9160 - val_loss: 0.3653\n",
      "Epoch 10/50\n",
      "\u001B[1m429/429\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m72s\u001B[0m 169ms/step - accuracy: 0.9395 - loss: 0.2036 - val_accuracy: 0.9154 - val_loss: 0.3800\n",
      "Epoch 11/50\n",
      "\u001B[1m429/429\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m72s\u001B[0m 169ms/step - accuracy: 0.9436 - loss: 0.1872 - val_accuracy: 0.9175 - val_loss: 0.3718\n",
      "Epoch 12/50\n",
      "\u001B[1m429/429\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m73s\u001B[0m 169ms/step - accuracy: 0.9475 - loss: 0.1715 - val_accuracy: 0.9184 - val_loss: 0.3731\n",
      "Epoch 13/50\n",
      "\u001B[1m429/429\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m74s\u001B[0m 172ms/step - accuracy: 0.9506 - loss: 0.1602 - val_accuracy: 0.9194 - val_loss: 0.3658\n",
      "Epoch 14/50\n",
      "\u001B[1m429/429\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m75s\u001B[0m 175ms/step - accuracy: 0.9535 - loss: 0.1488 - val_accuracy: 0.9197 - val_loss: 0.3821\n",
      "Epoch 14: early stopping\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " The analysis of the model's performance revealed that after just five training epochs, there were indications of overfitting. Despite this, the validation accuracy continued to rise gradually, while the loss tended to increase. Notwithstanding the presence of overfitting, the model demonstrated superior performance on the test set. Consequently, further refinement was undertaken for an additional three training epochs in order to optimize the test score as explained earlier. The objective was to enhance the model's generalization capability while maintaining its accuracy on unseen data. This approach aimed to mitigate overfitting and improve overall performance without compromising the model's effectiveness on new, previously unencountered instances.",
   "id": "d9febe1c165d04cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "----",
   "id": "37d18608cc3145c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The code sets up an early stopping callback to monitor validation accuracy, defines a sparse categorical accuracy metric, and creates an Adam optimizer with specific learning rate and beta values. Then, the model is compiled for training for the second time for refining the model accuracy using these parameters and sparse categorical crossentropy as the loss function.",
   "id": "dc8f8bbbd78ca550"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:05:22.601464Z",
     "start_time": "2024-06-13T18:05:22.595454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "round_two_early_stopping_callback = EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=3,\n",
    "        verbose=1,\n",
    "        mode=\"max\",\n",
    ")\n",
    "round_two_metric = SparseCategoricalAccuracy(name=\"accuracy\")\n",
    "round_two_optimizer = Adam(\n",
    "        learning_rate=5e-5,\n",
    "        beta_1=0.90,\n",
    "        beta_2=0.98,\n",
    "        epsilon=1e-9,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "        optimizer=round_two_optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[round_two_metric],\n",
    ")"
   ],
   "id": "70b5b615bd6b48f1",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "train the neural network for the refinement of the accuracy.",
   "id": "8317c9ea7b2b928b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:11:41.675633Z",
     "start_time": "2024-06-13T18:05:22.602231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.fit(\n",
    "        data_generator(original_data_train, target_data_train, batch_size_=BATCH_SIZE),\n",
    "        epochs=3,\n",
    "        steps_per_epoch=len(original_data_train) // BATCH_SIZE,\n",
    "        validation_data=data_generator(original_data_test, target_data_test, batch_size_=BATCH_SIZE),\n",
    "        validation_steps=5,\n",
    "        shuffle=True,\n",
    "        callbacks=[round_two_early_stopping_callback],\n",
    ")\n",
    "\n",
    "# save the model to the disk for later use.\n",
    "model.save(\"final_model.keras\")"
   ],
   "id": "7a9f3dd5a024503e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1718301945.849178 2788982 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "I0000 00:00:1718302017.516372 2788982 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_43', 40 bytes spill stores, 40 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_41', 484 bytes spill stores, 484 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_33', 4 bytes spill stores, 40 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m429/429\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 166ms/step - accuracy: 0.9607 - loss: 0.1228"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1718302090.529376 2788979 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m429/429\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m169s\u001B[0m 172ms/step - accuracy: 0.9607 - loss: 0.1228 - val_accuracy: 0.9233 - val_loss: 0.3586\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1718302093.002005 2788978 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "I0000 00:00:1718302157.019882 2788978 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_43', 40 bytes spill stores, 40 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_41', 4040 bytes spill stores, 4208 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function '__cuda_sm3x_div_rn_noftz_f32_slowpath', 52 bytes spill stores, 48 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_33', 4 bytes spill stores, 40 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m429/429\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m138s\u001B[0m 170ms/step - accuracy: 0.9663 - loss: 0.1025 - val_accuracy: 0.9218 - val_loss: 0.3826\n",
      "Epoch 3/3\n",
      "\u001B[1m429/429\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m71s\u001B[0m 166ms/step - accuracy: 0.9686 - loss: 0.0948 - val_accuracy: 0.9269 - val_loss: 0.3525\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Metrics",
   "id": "e9eb27172dffb49"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Let s be the source string and p your prediction. The quality of the results will be measured according to the following metric:\n",
    "\n",
    "1.  look for the longest substring w between s and p\n",
    "2.  compute |w|/max(|s|,|p|)\n",
    "\n",
    "If the match is exact, the score is 1.\n",
    "\n",
    "When computing the score, you should NOT consider the start and end tokens.\n",
    "\n"
   ],
   "id": "3a58847aeaf2428a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The longest common substring can be computed with the SequenceMatcher function of difflib, that allows a simple definition of our metric.",
   "id": "d32c0f2f93510811"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:11:41.679039Z",
     "start_time": "2024-06-13T18:11:41.676504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def score(s, p)->float:\n",
    "    \"\"\"\n",
    "    Calculate the similarity score between two strings using the SequenceMatcher algorithm.\n",
    "\n",
    "    The similarity score is defined as the size of the longest matching subsequence\n",
    "    divided by the length of the longer string.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    s : str\n",
    "        The first input string.\n",
    "    p : str\n",
    "        The second input string.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    score : float\n",
    "        The similarity score between the two input strings, ranging from 0 (no match) to 1 (perfect match).\n",
    "    \n",
    "    \"\"\"\n",
    "    match = SequenceMatcher(None, s, p).find_longest_match()\n",
    "    return match.size / max(len(p), len(s))"
   ],
   "id": "5dd010a3c56cb9ac",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:11:41.689632Z",
     "start_time": "2024-06-13T18:11:41.679981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_model_on_scrambled_inputs(\n",
    "        model_: keras.Model,\n",
    "        input_sequences: np.ndarray,\n",
    "        start_token: int = 3,\n",
    "        end_token: int = 2,\n",
    "        pad_token: int = 0\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Run a model on scrambled inputs and return the predicted output sequences.\n",
    "\n",
    "    This function takes an input sequence, processes it by scrambling the tokens based\n",
    "    on specified start token, end token, and padding token, and then passes this to a\n",
    "    provided Keras Model for prediction. The generated predictions are returned as output\n",
    "    sequences.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_ : keras.Model\n",
    "       The Keras model used for prediction. This should be a pre-trained model that accepts\n",
    "       input in the format required by this function.\n",
    "    input_sequences : np.ndarray\n",
    "       A 2D NumPy array representing the sequences to be processed and predicted upon. Each row\n",
    "       represents an individual sequence of tokens, with shape (batch size, sequence length).\n",
    "    start_token : int, optional\n",
    "       The token value used to initialize each output sequence. Default is 3.\n",
    "    end_token : int, optional\n",
    "       The token value that indicates the end of a sequence. This function stops processing\n",
    "       further tokens in a sequence once it encounters this token. Default is 2.\n",
    "    pad_token : int, optional\n",
    "       The token value used to fill out sequences shorter than the maximum sequence length.\n",
    "       Default is 0.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "       A 2D NumPy array containing the predicted output sequences. Shape will be identical to\n",
    "       that of input_sequences, where each row represents an individual sequence of tokens.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    - This function assumes a batch processing approach, where all inputs are processed in one go.\n",
    "     Therefore, the batch size determines the number of sequences handled simultaneously.\n",
    "    - The model's output is expected to be a probability distribution over possible tokens for each\n",
    "     position in a sequence. This function uses argmax to select the token with highest probability.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> from tensorflow import keras\n",
    "    >>> model = keras.models.Sequential()  # assume a pre-trained model\n",
    "    >>> input_sequences = np.array([[1, 2, 3, 4, 0], [5, 6, 7, 2, 0]])\n",
    "    >>> output_sequences = run_model_on_scrambled_inputs(model, input_sequences)\n",
    "   \"\"\"\n",
    "    batch_size, sequence_length = input_sequences.shape\n",
    "    mask_pad = np.where(input_sequences == pad_token, 0, 1)\n",
    "    input_sequences = input_sequences.reshape((batch_size, sequence_length))\n",
    "\n",
    "    # Initialize output sequences with the pad token and set the first token to the start token\n",
    "    output_sequences = np.full((batch_size, sequence_length), pad_token, dtype=np.int32)\n",
    "    output_sequences[:, 0] = start_token\n",
    "\n",
    "    # Preprocess sequences to handle end tokens and padding\n",
    "    for i in range(batch_size):\n",
    "        for j in range(sequence_length):\n",
    "            if input_sequences[i, j] == end_token:\n",
    "                output_sequences[i, j] = end_token\n",
    "                output_sequences[i, j + 1:] = pad_token\n",
    "                break\n",
    "\n",
    "    for i in range(1, sequence_length):\n",
    "        # Predict the next token for each position in the sequence\n",
    "        output_probs = model_.predict([input_sequences, output_sequences, mask_pad, mask_pad], verbose=0)\n",
    "        next_token_indices = np.argmax(output_probs[:, i - 1, :], axis=1)\n",
    "\n",
    "        # Add the predicted token to the output sequence\n",
    "        for j in range(batch_size):\n",
    "            # Stop if end token or pad token is reached\n",
    "            if output_sequences[j, i - 1] != end_token and output_sequences[j, i - 1] != pad_token:\n",
    "                output_sequences[j, i] = next_token_indices[j]\n",
    "\n",
    "        # Stop if all sequences have generated either the end token or the pad token\n",
    "        if np.all(next_token_indices == end_token) or np.all(next_token_indices == pad_token):\n",
    "            break\n",
    "\n",
    "    return output_sequences\n"
   ],
   "id": "39707eff4ee38ac1",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:11:41.700314Z",
     "start_time": "2024-06-13T18:11:41.690440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_special_tokens(sentence: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove special tokens \"<start>\" and \"<end>\" from a sentence.\n",
    "\n",
    "    This function takes a string as input, which typically represents a\n",
    "    sentence or text data. It removes the \"<start>\" and \"<end>\" special\n",
    "    tokens from the provided sentence.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sentence : str\n",
    "      The input sentence from which to remove the special tokens. It can be a string containing any characters including alphabets, digits, and punctuation marks.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "      The output sentence with \"<start>\" and \"<end>\" special tokens removed.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    This function uses the Python's built-in string replacement method to\n",
    "    remove the specified tokens from the input sentence.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> sentence = \"<start>This is a test sentence.<end>\"\n",
    "    >>> print(remove_special_tokens(sentence))\n",
    "    'This is a test sentence.'\n",
    "  \"\"\"\n",
    "    return sentence.replace(\"<start>\", \"\").replace(\"<end>\", \"\").strip()\n",
    "\n",
    "\n",
    "def calculate_score_for_test_sequences(\n",
    "        num_samples: int = None,\n",
    "        use_model: bool = False,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate scores for test sequences using either a model or base input data.\n",
    "    \n",
    "    This function iterates over the `test generator` to calculate scores for `test` sequences.\n",
    "    The number of samples used is determined by the `num_samples` parameter, and if not provided,\n",
    "    it defaults to the length of the test generator. For each batch of data, the function determines\n",
    "    whether to use a model or base input data to generate output sentences. It then calculates the score for each pair\n",
    "    of ordered and output sentences and appends it to a deque of scores. Finally, it prints statistics about the scores\n",
    "    and returns the scores as a numpy array.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    num_samples : int or None, optional\n",
    "        The number of samples to use for calculating scores. If not provided, defaults to the length of the test generator.\n",
    "    use_model : bool, optional\n",
    "        Whether to use a model to generate output sentences. If False, uses base input data instead. Default is False.\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    scores : numpy.ndarray\n",
    "        A 1-dimensional array containing the scores for each pair of ordered and output sentences.\n",
    "    \"\"\"\n",
    "    scores = collections.deque()\n",
    "\n",
    "    if num_samples is None:\n",
    "        num_samples = len(test_generator)\n",
    "\n",
    "    for batch_idx, ((scrambled_data, ordered_data, _, _), _) in enumerate(test_generator):\n",
    "        if num_samples <= 0:\n",
    "            break\n",
    "\n",
    "        ordered_sentences = detokenizer(ordered_data)\n",
    "        if use_model:\n",
    "            output_sentences = detokenizer(run_model_on_scrambled_inputs(model, scrambled_data))\n",
    "        else:\n",
    "            output_sentences = detokenizer(scrambled_data)\n",
    "\n",
    "        for ordered_sentence, output_sentence in zip(ordered_sentences, output_sentences):\n",
    "            ordered_sentence = remove_special_tokens(ordered_sentence)\n",
    "            output_sentence = remove_special_tokens(output_sentence)\n",
    "            scores.append(score(output_sentence, ordered_sentence))\n",
    "\n",
    "        num_samples -= ordered_data.shape[0]\n",
    "\n",
    "    scores = np.array(scores)\n",
    "    print(\"#\" * 50)\n",
    "    if use_model:\n",
    "        print(\"Model Score\")\n",
    "    else:\n",
    "        print(\"Base Score\")\n",
    "    print(\"#\" * 50)\n",
    "    print(f\"Average score: {np.mean(scores):.4f}\")\n",
    "    print(f\"Std dev score: {np.std(scores):.4f}\")\n",
    "    print(f\"Min score:     {min(scores):.4f}\")\n",
    "    print(f\"Median score:  {np.median(scores):.4f}\")\n",
    "    print(f\"Max score:     {max(scores):.4f}\")\n",
    "    print(\"#\" * 50)\n",
    "\n",
    "    return scores"
   ],
   "id": "da7930fd22d5b5ca",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:11:41.756187Z",
     "start_time": "2024-06-13T18:11:41.700994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# calculating the base score for the test data without the model for comparison\n",
    "_ = calculate_score_for_test_sequences()"
   ],
   "id": "1487de27e0ea67d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "Base Score\n",
      "##################################################\n",
      "Average score: 0.1865\n",
      "Std dev score: 0.0524\n",
      "Min score:     0.1008\n",
      "Median score:  0.1771\n",
      "Max score:     0.5085\n",
      "##################################################\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:13:01.147840Z",
     "start_time": "2024-06-13T18:11:41.756977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# calculating the score for the test data with the model for comparison\n",
    "_ = calculate_score_for_test_sequences(num_samples=4000, use_model=True)"
   ],
   "id": "20202b7454b12cd5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1718302303.919604 2824785 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 192 bytes spill stores, 192 bytes spill loads\n",
      "\n",
      "I0000 00:00:1718302303.945351 2824781 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 28 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "I0000 00:00:1718302304.086236 2824774 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 464 bytes spill stores, 412 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "Model Score\n",
      "##################################################\n",
      "Average score: 0.5254\n",
      "Std dev score: 0.2873\n",
      "Min score:     0.1099\n",
      "Median score:  0.4268\n",
      "Max score:     1.0000\n",
      "##################################################\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9a3a4acecdbf0545"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this project, I have with various configurations to optimize our model's performance and efficiency. The main configuration used as a \n",
    "baseline had 6 encoder and decoder layers resulting in 14 million parameters and achieved a score of 0.51 on the training set.\n",
    "\n",
    "## Model Parameter Analysis\n",
    "\n",
    "I modified the number of encoder and decoder layers to 10 instead of 6, which increased the model parameters to approximately 14M. The model's performance \n",
    "with this configuration was found to be similar to that of the baseline, indicating that increasing complexity may not necessarily lead to improved accuracy in all cases.\n",
    "\n",
    "Another configuration involved changing the embedding dimension from the original value to 256. This resulted in a model with approximately 20M parameters with the score of 0.523. However, the early stop trigger was activated on the 11th epoch, suggesting potential overfitting or instability issues with this larger model.\n",
    "\n",
    "A more efficient version of the previous configuration was achieved by reducing the embedding dimension to 128. This resulted in a model with half the number of parameters (around 10M) while maintaining approximately the same level of accuracy as the 20M model, and with faster inference time due to the smaller model size.\n",
    "\n",
    "## Attention Heads Analysis\n",
    "\n",
    "I also experimented with changing the number of attention heads from the original value to 20. However, this configuration resulted in a score of only 0.50, which was slightly lower than that of the 20M model. Furthermore, overfitting started occurring after the 14th epoch, highlighting the importance of careful tuning to prevent such issues.\n",
    "\n",
    "## Intermediate Dimension Analysis\n",
    "\n",
    "Modifying the intermediate dimension to 3000 did not significantly impact performance; however, it did result in a model with approximately 14M parameters, similar to other configurations.\n",
    "\n",
    "## Dense Layers and Loss Function Analysis\n",
    "\n",
    "Adding additional dense layers before the main dense layer of the neural network resulted in a model with 11.5M parameters. However, this configuration did not significantly improve performance, suggesting that adding complexity may not always lead to improved results.\n",
    "\n",
    "I used the categorical crossentropy loss function for all configurations and one-hot encoded the target data. However, using sparse categorical crossentropy instead led to improvements in training time and reduced RAM usage without compromising performance.\n",
    "\n",
    "When label smoothing of 0.1 was applied with the categorical crossentropy loss function, it performed similarly to the 20M model, further demonstrating that careful tuning can lead to competitive results without requiring a more complex model.\n",
    "\n",
    "## Final Configuration\n",
    "\n",
    "For our final configuration, I used an embedding size of 128 and layer normalization before the final dense layer. This resulted in a model with approximately 11M parameters, which performed similarly to the 20M model, showcasing that optimal results can be achieved with careful configuration tuning and the right balance between complexity and efficiency."
   ],
   "id": "31ccc4c81f822f34"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
